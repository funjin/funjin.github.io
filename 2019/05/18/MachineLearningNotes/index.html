<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    <script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>
    <link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">
  
  

  <!-- PACE Progress Bar START -->

  
  <title>machinelearningnotes | Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta name="description" content="第11周-过拟合问题会议讨论内容：1.介绍激活函数添加非线性成分进去，解决非线性问题（例如多元分类）Sigmoid：梯度饱和时学习进度为0Swish函数：以sigmoid为基地的修正对层数比较多的模型比较好Tanh函数：相对sigmoid收敛更快Relu函数：变种relu函数，softplus函数relu稀疏性表现很可靠 2.讲解梯度消失，梯度爆炸介绍sigmoid函数：目前层数比较多用的都是re">
<meta property="og:type" content="article">
<meta property="og:title" content="MachineLearningNotes">
<meta property="og:url" content="http://yoursite.com/2019/05/18/MachineLearningNotes/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="第11周-过拟合问题会议讨论内容：1.介绍激活函数添加非线性成分进去，解决非线性问题（例如多元分类）Sigmoid：梯度饱和时学习进度为0Swish函数：以sigmoid为基地的修正对层数比较多的模型比较好Tanh函数：相对sigmoid收敛更快Relu函数：变种relu函数，softplus函数relu稀疏性表现很可靠 2.讲解梯度消失，梯度爆炸介绍sigmoid函数：目前层数比较多用的都是re">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/bc9fe3ab.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/31031f8b.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c7b475a8.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c95639d2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/9aa52dee.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/614b201c.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/6c533ebd.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/4a5a7aae.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/ffe97136.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c112c635.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/06d68a5f.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/e5f1dd20.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/e08a1450.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/cd5b3c20.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/571bf7ba.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c05fcf1d.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/6fc40919.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/fa5bdf21.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/493c495c.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/39c25f99.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/436f3f91.png">
<meta property="og:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/9dd12e10.png">
<meta property="og:updated_time" content="2019-05-17T20:35:41.829Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MachineLearningNotes">
<meta name="twitter:description" content="第11周-过拟合问题会议讨论内容：1.介绍激活函数添加非线性成分进去，解决非线性问题（例如多元分类）Sigmoid：梯度饱和时学习进度为0Swish函数：以sigmoid为基地的修正对层数比较多的模型比较好Tanh函数：相对sigmoid收敛更快Relu函数：变种relu函数，softplus函数relu稀疏性表现很可靠 2.讲解梯度消失，梯度爆炸介绍sigmoid函数：目前层数比较多用的都是re">
<meta name="twitter:image" content="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/bc9fe3ab.png">
  
    <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/hiero.css">
  <link rel="stylesheet" href="/css/glyphs.css">
  
    <link rel="stylesheet" href="/css/vdonate.css">
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">
  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
</head>
</html>
<script>
var themeMenus = {};

  themeMenus["/"] = "首页"; 

  themeMenus["/archives"] = "归档"; 

  themeMenus["/categories"] = "分类"; 

  themeMenus["/tags"] = "标签"; 

  themeMenus["/about"] = "关于"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" rel="home">
                <img style="margin-bottom: 10px;" width="124px" height="124px" alt="Hike News" src=" /css/images/rufui.jpg">
              </a>
            
          </h1>

          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class href="/about">关于</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt src>
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/rurushiu.jpg,https://source.unsplash.com/collection/954550/1920x1080".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-MachineLearningNotes" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      MachineLearningNotes
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/05/18/MachineLearningNotes/" class="article-date">
	  <time datetime="2019-05-17T20:01:15.000Z" itemprop="datePublished">五月 18, 2019</time>
	</a>

      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="第11周-过拟合问题"><a href="#第11周-过拟合问题" class="headerlink" title="第11周-过拟合问题"></a>第11周-过拟合问题</h1><h2 id="会议讨论内容："><a href="#会议讨论内容：" class="headerlink" title="会议讨论内容："></a>会议讨论内容：</h2><h3 id="1-介绍激活函数"><a href="#1-介绍激活函数" class="headerlink" title="1.介绍激活函数"></a>1.介绍激活函数</h3><p>添加非线性成分进去，解决非线性问题（例如多元分类）<br>Sigmoid：梯度饱和时学习进度为0<br>Swish函数：以sigmoid为基地的修正对层数比较多的模型比较好<br>Tanh函数：相对sigmoid收敛更快<br>Relu函数：变种relu函数，softplus函数relu稀疏性表现很可靠</p>
<h3 id="2-讲解梯度消失，梯度爆炸"><a href="#2-讲解梯度消失，梯度爆炸" class="headerlink" title="2.讲解梯度消失，梯度爆炸"></a>2.讲解梯度消失，梯度爆炸</h3><p>介绍sigmoid函数：目前层数比较多用的都是relu，浅层三四层的是sigmoid或tanh函数<br>介绍softmax函数：从二分类将其，用sigmoid函数来替代单位跃迁函数</p>
<h3 id="3-下个阶段的学习"><a href="#3-下个阶段的学习" class="headerlink" title="3.下个阶段的学习"></a>3.下个阶段的学习</h3><p>过拟合的概念：激励函数的选择，正则化等都是为了解决过拟合。</p>
<h2 id="过拟合带来的问题：繁华能力很差，仅限于在训练样本中效果很好，在对其他数据集（训练集之外）时效果很差，很多神经网络中调参都是为了解决过拟合问题"><a href="#过拟合带来的问题：繁华能力很差，仅限于在训练样本中效果很好，在对其他数据集（训练集之外）时效果很差，很多神经网络中调参都是为了解决过拟合问题" class="headerlink" title="过拟合带来的问题：繁华能力很差，仅限于在训练样本中效果很好，在对其他数据集（训练集之外）时效果很差，很多神经网络中调参都是为了解决过拟合问题"></a>过拟合带来的问题：繁华能力很差，仅限于在训练样本中效果很好，在对其他数据集（训练集之外）时效果很差，很多神经网络中调参都是为了解决过拟合问题</h2><h2 id="过拟合解决方案：："><a href="#过拟合解决方案：：" class="headerlink" title="过拟合解决方案：："></a>过拟合解决方案：：</h2><h2 id="一、增加数据量"><a href="#一、增加数据量" class="headerlink" title="一、增加数据量"></a>一、增加数据量</h2><hr>
<h2 id="二、加正则项"><a href="#二、加正则项" class="headerlink" title="二、加正则项"></a>二、加正则项</h2><p>损失函数后面加上正则项，解决函数过于扭曲，某些权值太大或者变化太大时会比较扭曲出现过拟合；在损失函数中将权值考虑过来，对过大话的权值惩罚，包括L1和L2两种</p>
<p>现在观察线性回归中的模型<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/bc9fe3ab.png" alt="bc9fe3ab.png"></p>
<h2 id="加入这个项就可以使函数更加平滑"><a href="#加入这个项就可以使函数更加平滑" class="headerlink" title="加入这个项就可以使函数更加平滑"></a>加入这个项就可以使函数更加平滑</h2><p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/31031f8b.png" alt="31031f8b.png"><br>如上图，假如对某个xi作一定变化Δxi，那么y相应的变化wiΔxi，如果wi更小，那么y对x越不敏感，函数也就更加平滑</p>
<h2 id="那么为啥平滑的函数更好呢？"><a href="#那么为啥平滑的函数更好呢？" class="headerlink" title="那么为啥平滑的函数更好呢？"></a>那么为啥平滑的函数更好呢？</h2><p>可以这样理解：如果在测试时有些额外的噪声影响了输入xi，那么一个更加平滑的函数会有较小影响，因此更适用泛化的数据</p>
<h2 id="现在可以看看上面加上正规化之后的结果"><a href="#现在可以看看上面加上正规化之后的结果" class="headerlink" title="现在可以看看上面加上正规化之后的结果"></a>现在可以看看上面加上正规化之后的结果</h2><p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c7b475a8.png" alt="c7b475a8.png"><br>总之要找到在测试数据上loss最低的点</p>
<p>核心思想<br>L1 / L2 正规化 (Regularization)<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c95639d2.png" alt="c95639d2.png"><br>我们拿 L2正规化来探讨一下, 机器学习的过程是一个 通过修改参数 theta 来减小误差的过程, 可是在减小误差的时候非线性越强的参数, 比如在 x^3 旁边的 theta 4 就会被修改得越多, 因为如果使用非线性强的参数就能使方程更加曲折, 也就能更好的拟合上那些分布的数据点. Theta 4 说, 瞧我本事多大, 就让我来改变模型, 来拟合所有的数据吧, 可是它这种态度招到了误差方程的强烈反击, 误差方程就说: no no no no, 我们是一个团队, 虽然你厉害, 但也不能仅仅靠你一个人, 万一你错了, 我们整个团队的效率就突然降低了, 我得 hold 住那些在 team 里独出风头的人. 这就是整套正规化算法的核心思想. 那 L1, L2 正规化又有什么不同呢?</p>
<p>图像化<br>L1 / L2 正规化 (Regularization)<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/9aa52dee.png" alt="9aa52dee.png"><br>想象现在只有两个参数 theta1 theta2 要学, 蓝色的圆心是误差最小的地方, 而每条蓝线上的误差都是一样的. 正规化的方程是在黄线上产生的额外误差(也能理解为惩罚度), 在黄圈上的额外误差也是一样. 所以在蓝线和黄线 交点上的点能让两个误差的合最小. 这就是 theta1 和 theta2 正规化后的解. 要提到另外一点是, 使用 L1 的方法, 我们很可能得到的结果是只有 theta1 的特征被保留, 所以很多人也用 l1 正规化来挑选对结果贡献最大的重要特征. 但是 l1 的结并不是稳定的. 比如用批数据训练, 每次批数据都会有稍稍不同的误差曲线,</p>
<p>L1 / L2 正规化 (Regularization)<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/614b201c.png" alt="614b201c.png"><br>L2 针对于这种变动, 白点的移动不会太大, 而 L1的白点则可能跳到许多不同的地方 , 因为这些地方的总误差都是差不多的. 侧面说明了 L1 解的不稳定性.</p>
<p>统一表达形式<br>L1 / L2 正规化 (Regularization)<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/6c533ebd.png" alt="6c533ebd.png"><br>最后,为了控制这种正规化的强度, 我们会加上一个参数 lambda, 并且通过 交叉验证 cross validation 来选择比较好的 lambda. 这时, 为了统一化这类型的正规化方法, 我们还会使用 p 来代表对参数的正规化程度. 这就是这一系列正规化方法的最终的表达形式啦.</p>
<h2 id="三、Drop-out随机选择算法"><a href="#三、Drop-out随机选择算法" class="headerlink" title="三、Drop-out随机选择算法"></a>三、Drop-out随机选择算法</h2><p>Drop-out随机选择神经元或者忽略神经元，进而使网络变得不完整，这样来解决过拟合问题。某些盛景园过于依赖W（权重）使得参数很大。这样做是的神经元稀疏起来，不会过于依赖某些W</p>
<p>Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p>
<p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/4a5a7aae.png" alt="4a5a7aae.png"></p>
<ol>
<li>Dropout简介<br>1.1 Dropout出现的原因<br>在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</li>
</ol>
<p>过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。</p>
<p>综上所述，训练深度神经网络的时候，总是会遇到两大缺点：</p>
<p>（1）容易过拟合</p>
<p>（2）费时</p>
<p>Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。</p>
<p>1.2 什么是Dropout<br>在2012年，Hinton在其论文《Improving neural networks by preventing co-adaptation of feature detectors》中提出Dropout。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。</p>
<p>在2012年，Alex、Hinton在其论文《ImageNet Classification with Deep Convolutional Neural Networks》中用到了Dropout算法，用于防止过拟合。并且，这篇论文提到的AlexNet网络模型引爆了神经网络应用热潮，并赢得了2012年图像识别大赛冠军，使得CNN成为图像分类上的核心算法模型。</p>
<p>随后，又有一些关于Dropout的文章《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》、《Improving Neural Networks with Dropout》、《Dropout as data augmentation》。</p>
<p>从上面的论文中，我们能感受到Dropout在深度学习中的重要性。那么，到底什么是Dropout呢？</p>
<p>Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p>
<p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。</p>
<p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/ffe97136.png" alt="ffe97136.png"><br>图1：使用Dropout的神经网络模型</p>
<ol start="2">
<li>Dropout工作流程及使用<br>2.1 Dropout具体工作流程<br>假设我们要训练这样一个神经网络，如图2所示。</li>
</ol>
<p>图2：标准的神经网络</p>
<p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c112c635.png" alt="c112c635.png"><br>输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：</p>
<p>（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）</p>
<p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/06d68a5f.png" alt="06d68a5f.png"></p>
<pre><code>图3：部分临时被删除的神经元
</code></pre><p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。</p>
<p>（3）然后继续重复这一过程：</p>
<p>. 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）<br>. 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。<br>. 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。<br>不断重复这一过程。</p>
<p>2.2 Dropout在神经网络中的使用<br>Dropout的具体工作流程上面已经详细的介绍过了，但是具体怎么让某些神经元以一定的概率停止工作（就是被删除掉）？代码层面如何实现呢？</p>
<p>下面，我们具体讲解一下Dropout代码层面的一些公式推导及代码实现思路。</p>
<p>（1）在训练模型阶段</p>
<p>无可避免的，在训练网络的每个单元都要添加一道概率流程。<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/e5f1dd20.png" alt="e5f1dd20.png"></p>
<p>图4：标准网络和带有Dropout网络的比较</p>
<p>对应的公式变化如下：</p>
<p> . 没有Dropout的网络计算公式：</p>
<pre><code>![cbd7f4fe.png](https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/cbd7f4fe.png)   
</code></pre><p>. 采用Dropout的网络计算公式：<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/e08a1450.png" alt="e08a1450.png"><br>上面公式中Bernoulli函数是为了生成概率r向量，也就是随机生成一个0、1的向量。</p>
<p>代码层面实现让某个神经元以概率p停止工作，其实就是让它的激活函数值以概率p变为0。比如我们某一层网络神经元的个数为1000个，其激活函数输出值为y1、y2、y3、……、y1000，我们dropout比率选择0.4，那么这一层神经元经过dropout后，1000个神经元中会有大约400个的值被置为0。</p>
<p>注意： 经过上面屏蔽掉某些神经元，使其激活值为0以后，我们还需要对向量y1……y1000进行缩放，也就是乘以1/(1-p)。如果你在训练的时候，经过置0后，没有对y1……y1000进行缩放（rescale），那么在测试的时候，就需要对权重进行缩放，操作如下。</p>
<p>（2）在测试模型阶段</p>
<p>预测模型的时候，每一个神经单元的权重参数要乘以概率p。</p>
<p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/cd5b3c20.png" alt="cd5b3c20.png"><br>图5：预测模型时Dropout的操作<br>测试阶段Dropout公式：<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/571bf7ba.png" alt="571bf7ba.png"></p>
<ol start="3">
<li>为什么说Dropout可以解决过拟合？<br>（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</li>
</ol>
<p>（2）减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</p>
<p>（3）Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</p>
<ol start="4">
<li>Dropout在Keras中的源码分析<br>下面，我们来分析Keras中Dropout实现源码。</li>
</ol>
<p>Keras开源项目GitHub地址为：</p>
<p><a href="https://github.com/fchollet/keras/tree/master/keras" target="_blank" rel="noopener">https://github.com/fchollet/keras/tree/master/keras</a></p>
<p>其中Dropout函数代码实现所在的文件地址：</p>
<p><a href="https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py" target="_blank" rel="noopener">https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py</a></p>
<p>Dropout实现函数如下：<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/c05fcf1d.png" alt="c05fcf1d.png"></p>
<p>图6：Keras中实现Dropout功能</p>
<p>我们对keras中Dropout实现函数做一些修改，让dropout函数可以单独运行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="comment"># dropout函数的实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, level)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">0.</span> <span class="keyword">or</span> level &gt;= <span class="number">1</span>: <span class="comment">#level是概率值，必须在0~1之间</span></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Dropout level must be in interval [0, 1[.'</span>)</span><br><span class="line">    retain_prob = <span class="number">1.</span> - level</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样</span></span><br><span class="line">    <span class="comment"># 硬币 正面的概率为p，n表示每个神经元试验的次数</span></span><br><span class="line">    <span class="comment"># 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。</span></span><br><span class="line">    random_tensor = np.random.binomial(n=<span class="number">1</span>, p=retain_prob, size=x.shape) <span class="comment">#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了</span></span><br><span class="line">    print(random_tensor)</span><br><span class="line"> </span><br><span class="line">    x *= random_tensor</span><br><span class="line">    print(x)</span><br><span class="line">    x /= retain_prob</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line"><span class="comment">#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  </span></span><br><span class="line">x=np.asarray([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],dtype=np.float32)</span><br><span class="line">dropout(x,<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure></p>
<p>函数中，x是本层网络的激活值。Level就是dropout就是每个神经元要被丢弃的概率。</p>
<p>注意： Keras中Dropout的实现，是屏蔽掉某些神经元，使其激活值为0以后，对激活值向量x1……x1000进行放大，也就是乘以1/(1-p)。</p>
<p>思考：上面我们介绍了两种方法进行Dropout的缩放，那么Dropout为什么需要进行缩放呢？</p>
<p>因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。</p>
<h2 id="对于这个问题，我还是没搞明白。"><a href="#对于这个问题，我还是没搞明白。" class="headerlink" title="对于这个问题，我还是没搞明白。"></a>对于这个问题，我还是没搞明白。</h2><p>不过好像可以参考cs231n<br><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">inverted dropout</a><br>总结：</p>
<p>当前Dropout被大量利用于全连接网络，而且一般认为设置为0.5或者0.3，而在卷积网络隐藏层中由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，Dropout策略在卷积网络隐藏层中使用较少。总体而言，Dropout是一个超参，需要根据具体的网络、具体的应用领域进行尝试。</p>
<hr>
<h2 id="四、批标准化（BN）"><a href="#四、批标准化（BN）" class="headerlink" title="四、批标准化（BN）"></a>四、批标准化（BN）</h2><h3 id="一、背景意义"><a href="#一、背景意义" class="headerlink" title="一、背景意义"></a>一、背景意义</h3><p>本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。</p>
<p>近年来深度学习捷报连连、声名鹊起，随机梯度下架成了训练深度网络的主流方法。尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。那么学完这篇文献之后，你可以不需要那么刻意的慢慢调整参数。BN算法（Batch Normalization）其强大之处如下：</p>
<p>(1)你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；</p>
<p>(2)你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；</p>
<p>(3)再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；</p>
<p>(4)可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。</p>
<p>开始讲解算法前，先来思考一个问题：我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。</p>
<p>对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。</p>
<p>我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch  Normalization，这个牛逼算法的诞生。</p>
<h3 id="二、初识BN-Batch-Normalization"><a href="#二、初识BN-Batch-Normalization" class="headerlink" title="二、初识BN(Batch  Normalization)"></a>二、初识BN(Batch  Normalization)</h3><h4 id="1、BN概述"><a href="#1、BN概述" class="headerlink" title="1、BN概述"></a>1、BN概述</h4><p>就像激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层。在前面我们提到网络除了输出层外，其它层因为低层网络在训练的时候更新了参数，而引起后面层输入数据分布的变化。这个时候我们可能就会想，如果在每一层输入的时候，再加个预处理操作那该有多好啊，比如网络第三层输入数据X3(X3表示网络第三层的输入数据)把它归一化至：均值0、方差为1，然后再输入第三层计算，这样我们就可以解决前面所提到的“Internal Covariate Shift”的问题了。</p>
<p>而事实上，paper的算法本质原理就是这样：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。不过文献归一化层，可不像我们想象的那么简单，它是一个可学习、有参数的网络层。既然说到数据预处理，下面就先来复习一下最强的预处理方法：白化。</p>
<h4 id="2、预处理操作选择"><a href="#2、预处理操作选择" class="headerlink" title="2、预处理操作选择"></a>2、预处理操作选择</h4><p>说到神经网络输入数据预处理，最好的算法莫过于白化预处理。然而白化计算量太大了，很不划算，还有就是白化不是处处可微的，所以在深度学习中，其实很少用到白化。经过白化预处理后，数据满足条件：a、特征之间的相关性降低，这个就相当于pca；b、数据均值、标准差归一化，也就是使得每一维特征均值为0，标准差为1。如果数据特征维数比较大，要进行PCA，也就是实现白化的第1个要求，是需要计算特征向量，计算量非常大，于是为了简化计算，作者忽略了第1个要求，仅仅使用了下面的公式进行预处理，也就是近似白化预处理：<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/6fc40919.png" alt="6fc40919.png"></p>
<p>公式简单粗糙，但是依旧很牛逼。因此后面我们也将用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch 随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。</p>
<h3 id="三、BN算法实现"><a href="#三、BN算法实现" class="headerlink" title="三、BN算法实现"></a>三、BN算法实现</h3><h4 id="1、BN算法概述"><a href="#1、BN算法概述" class="headerlink" title="1、BN算法概述"></a>1、BN算法概述</h4><p>经过前面简单介绍，这个时候可能我们会想当然的以为：好像很简单的样子，不就是在网络中间层数据做一个归一化处理嘛，这么简单的想法，为什么之前没人用呢？然而其实实现起来并不是那么简单的。其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数γ、β，这就是算法关键之处：<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/fa5bdf21.png" alt="fa5bdf21.png"></p>
<p>每一个神经元xk都会有一对这样的参数γ、β。这样其实当：<br><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/493c495c.png" alt="493c495c.png"><br>、</p>
<p>是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。最后Batch Normalization网络层的前向传导过程公式就是：</p>
<p> <img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/39c25f99.png" alt="39c25f99.png"></p>
<p>上面的公式中m指的是mini-batch size。</p>
<h4 id="2、源码实现"><a href="#2、源码实现" class="headerlink" title="2、源码实现"></a>2、源码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = K.mean(X, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)<span class="comment">#计算均值</span></span><br><span class="line">           std = K.std(X, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)<span class="comment">#计算标准差</span></span><br><span class="line">           X_normed = (X - m) / (std + self.epsilon)<span class="comment">#归一化</span></span><br><span class="line">           out = self.gamma * X_normed + self.beta<span class="comment">#重构变换</span></span><br></pre></td></tr></table></figure>
<p>上面的x是一个二维矩阵，对于源码的实现就几行代码而已，轻轻松松。</p>
<h4 id="3、实战使用"><a href="#3、实战使用" class="headerlink" title="3、实战使用"></a>3、实战使用</h4><p>(1)可能学完了上面的算法，你只是知道它的一个训练过程，一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。因此测试样本，前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值u、和标准差都是固定不变的。我们可以采用这些数值来作为测试样本所需要的均值、标准差，于是最后测试阶段的u和σ 计算公式如下：</p>
<p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/436f3f91.png" alt="436f3f91.png"></p>
<p>上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是：</p>
<p><img src="https://raw.githubusercontent.com/funjin/funjin.github.io/master/attachments/9dd12e10.png" alt="9dd12e10.png"></p>
<p>(2)根据文献说，BN可以应用于一个神经网络的任何神经元上。文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的：</p>
<p>z=g(Wu+b)</p>
<p>也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：</p>
<p>z=g(BN(Wu+b))</p>
<p>其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：</p>
<p>z=g(BN(Wu))</p>
<h3 id="四、Batch-Normalization在CNN中的使用"><a href="#四、Batch-Normalization在CNN中的使用" class="headerlink" title="四、Batch Normalization在CNN中的使用"></a>四、Batch Normalization在CNN中的使用</h3><p>通过上面的学习，我们知道BN层是对于每个神经元做归一化处理，甚至只需要对某一个神经元进行归一化，而不是对一整层网络的神经元进行归一化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？假如某一层卷积层有6个特征图，每个特征图的大小是100<em>100，这样就相当于这一层网络有6</em>100<em>100个神经元，如果采用BN，就会有6</em>100*100个参数γ、β，这样岂不是太恐怖了。因此卷积层上的BN使用，其实也是使用了类似权值共享的策略，把一整张特征图当做一个神经元进行处理。</p>
<p>卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch sizes，f为特征图个数，p、q分别为特征图的宽高。在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m<em>p</em>q，于是对于每个特征图都只有一对可学习参数：γ、β。说白了吧，这就是相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。下面是来自于keras卷积层的BN实现一小段主要源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">input_shape = self.input_shape</span><br><span class="line">            reduction_axes = list(range(len(input_shape)))</span><br><span class="line">            <span class="keyword">del</span> reduction_axes[self.axis]</span><br><span class="line">            broadcast_shape = [<span class="number">1</span>] * len(input_shape)</span><br><span class="line">            broadcast_shape[self.axis] = input_shape[self.axis]</span><br><span class="line">            <span class="keyword">if</span> train:</span><br><span class="line">                m = K.mean(X, axis=reduction_axes)</span><br><span class="line">                brodcast_m = K.reshape(m, broadcast_shape)</span><br><span class="line">                std = K.mean(K.square(X - brodcast_m) + self.epsilon, axis=reduction_axes)</span><br><span class="line">                std = K.sqrt(std)</span><br><span class="line">                brodcast_std = K.reshape(std, broadcast_shape)</span><br><span class="line">                mean_update = self.momentum * self.running_mean + (<span class="number">1</span>-self.momentum) * m</span><br><span class="line">                std_update = self.momentum * self.running_std + (<span class="number">1</span>-self.momentum) * std</span><br><span class="line">                self.updates = [(self.running_mean, mean_update),</span><br><span class="line">                                (self.running_std, std_update)]</span><br><span class="line">                X_normed = (X - brodcast_m) / (brodcast_std + self.epsilon)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                brodcast_m = K.reshape(self.running_mean, broadcast_shape)</span><br><span class="line">                brodcast_std = K.reshape(self.running_std, broadcast_shape)</span><br><span class="line">                X_normed = ((X - brodcast_m) /</span><br><span class="line">                            (brodcast_std + self.epsilon))</span><br><span class="line">            out = K.reshape(self.gamma, broadcast_shape) * X_normed + K.reshape(self.beta, broadcast_shape)</span><br></pre></td></tr></table></figure>
<p>个人总结：2015年个人最喜欢深度学习的一篇paper就是Batch Normalization这篇文献，采用这个方法网络的训练速度快到惊人啊，感觉训练速度是以前的十倍以上，再也不用担心自己这破电脑每次运行一下，训练一下都要跑个两三天的时间。另外这篇文献跟空间变换网络《Spatial Transformer Networks》的思想神似啊，都是一个变换网络层。</p>
<p>参考文献：</p>
<p>1、《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》</p>
<p>2、《Spatial Transformer Networks》</p>
<p>3、<a href="https://github.com/fchollet/keras" target="_blank" rel="noopener">https://github.com/fchollet/keras</a></p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
      
      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: '打赏支持', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: 'https://raw.githubusercontent.com/iTimeTraveler/iTimeTraveler.github.io/site/source/about/donate/images/WeChanQR.png',
  alipayImage: 'https://raw.githubusercontent.com/iTimeTraveler/iTimeTraveler.github.io/site/source/about/donate/images/AliPayQR.jpg'
});
</script>
      
            
      
        
	<div id="comment">
	
	<!-- 多说评论框 start -->
	 <div class="ds-thread" data-thread-key="/2019/05/18/MachineLearningNotes/" data-title="MachineLearningNotes" data-url="http://yoursite.com/2019/05/18/MachineLearningNotes/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"iTimeTraveler"};
	  (function() {
	    var ds = document.createElement('script');
	    ds.type = 'text/javascript';ds.async = true;
	    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
	    ds.charset = 'UTF-8';
	    (document.getElementsByTagName('head')[0] 
	     || document.getElementsByTagName('body')[0]).appendChild(ds);
	  })();
	  </script>
	<!-- 多说公共JS代码 end -->
	
	</div>
	<link rel="stylesheet" href="/css/comment-ds.css">


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2019/05/18/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article" style="overflow-y: scroll; max-width: 28%;">
    <strong class="toc-title">文章目录</strong>
    
      <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第11周-过拟合问题"><span class="nav-number">1.</span> <span class="nav-text">第11周-过拟合问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#会议讨论内容："><span class="nav-number">1.1.</span> <span class="nav-text">会议讨论内容：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-介绍激活函数"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.介绍激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-讲解梯度消失，梯度爆炸"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.讲解梯度消失，梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-下个阶段的学习"><span class="nav-number">1.1.3.</span> <span class="nav-text">3.下个阶段的学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#过拟合带来的问题：繁华能力很差，仅限于在训练样本中效果很好，在对其他数据集（训练集之外）时效果很差，很多神经网络中调参都是为了解决过拟合问题"><span class="nav-number">1.2.</span> <span class="nav-text">过拟合带来的问题：繁华能力很差，仅限于在训练样本中效果很好，在对其他数据集（训练集之外）时效果很差，很多神经网络中调参都是为了解决过拟合问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#过拟合解决方案：："><span class="nav-number">1.3.</span> <span class="nav-text">过拟合解决方案：：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一、增加数据量"><span class="nav-number">1.4.</span> <span class="nav-text">一、增加数据量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、加正则项"><span class="nav-number">1.5.</span> <span class="nav-text">二、加正则项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#加入这个项就可以使函数更加平滑"><span class="nav-number">1.6.</span> <span class="nav-text">加入这个项就可以使函数更加平滑</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#那么为啥平滑的函数更好呢？"><span class="nav-number">1.7.</span> <span class="nav-text">那么为啥平滑的函数更好呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#现在可以看看上面加上正规化之后的结果"><span class="nav-number">1.8.</span> <span class="nav-text">现在可以看看上面加上正规化之后的结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、Drop-out随机选择算法"><span class="nav-number">1.9.</span> <span class="nav-text">三、Drop-out随机选择算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对于这个问题，我还是没搞明白。"><span class="nav-number">1.10.</span> <span class="nav-text">对于这个问题，我还是没搞明白。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、批标准化（BN）"><span class="nav-number">1.11.</span> <span class="nav-text">四、批标准化（BN）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、背景意义"><span class="nav-number">1.11.1.</span> <span class="nav-text">一、背景意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、初识BN-Batch-Normalization"><span class="nav-number">1.11.2.</span> <span class="nav-text">二、初识BN(Batch  Normalization)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、BN概述"><span class="nav-number">1.11.2.1.</span> <span class="nav-text">1、BN概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、预处理操作选择"><span class="nav-number">1.11.2.2.</span> <span class="nav-text">2、预处理操作选择</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、BN算法实现"><span class="nav-number">1.11.3.</span> <span class="nav-text">三、BN算法实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、BN算法概述"><span class="nav-number">1.11.3.1.</span> <span class="nav-text">1、BN算法概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、源码实现"><span class="nav-number">1.11.3.2.</span> <span class="nav-text">2、源码实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、实战使用"><span class="nav-number">1.11.3.3.</span> <span class="nav-text">3、实战使用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、Batch-Normalization在CNN中的使用"><span class="nav-number">1.11.4.</span> <span class="nav-text">四、Batch Normalization在CNN中的使用</span></a></li></ol></li></ol></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 Blog All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>


<script src="/js/scripts.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="https://dnqof95d40fo6.cloudfront.net/atw7f8.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
	<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/click.js"></script>
  <script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
